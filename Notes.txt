Why not file system or why db?
1. Data inconsistency and Redundancy...
2. Concurrent data access problems...
3. Not possible to make data relationships b/w data...
4. Isolation problems(for example: file1.dat and file2.txt)...
5. Faster data access problems and manipulation problems with file system...

(Although all above can be done by writing programs for file system but instead of 
writing it for every file, we prefer centralized data management with the help of DBMS...)

DBMS basically provides abstracted view of the data to different users...

Three Schema Arcitecture(Provides personalised view of data to different users)...

1. Physical Schema - 
1.1 - Its a blueprint which tells how the data is stored in the disks or at the physical level...
1.2 - We must define algos which makes data access faster like storing the data in N-ary tree...

2. Conceptual/Logical Schema -
2.1 - Its a blueprint which tells what data are stored in DB and what relationships exist among data...
2.2 - Its independent from physical level...
2.3 - It also controls consistency constraints...
2.4 - Its goal is ease to use...

3. External/View Schema -
3.1 - It provides different view of the data or subschemas to different end users...
3.2 - It also provides security mechanism to provides data access to only authorized users...

Data Base Languages - 
1. DDL to specify db schema and also specify consistency constraints...
2. DML for retrieval, deletion, updation and insertion of data...

Database Application Architectures - 
1. In T1 architecture client, DB and DB server all reside on the same machine...
2. In T2 architecture client and server reside on different machines and communication is facilitated by JDBC or ODBC 
or some interface-> Has limited scalability when dealing with multiple clients and security issues are also there...
3. In T3 architecture client,server and db reside on different machines-> Data integrity is maintained and highly scalable...

Data Models - Visualization of relationships between different entities and attributes...

1. Each entity is uniquely identifiable...
2. Entity/Entity Set is a real world object...

E-R Model - It is the visualization of objects or entities and relationships between them...

Types of Attributes - Derived, Composite, multivalued, singleValued...

NULL Value - 
1. When a value is not assigned to an attribute -> May indicate inconsistency, deliberately leaving...

Strong Entity - It has independent existence and is identifiable by a primary key...
Weak Entity - It has dependency on Strong Entity and has no primary key...

Weak Entity: Exists only because of the parent table.
Strong Entity: Exists and is identifiable on its own, even if related to a parent table.

Degree Of Relation tells how much entities are taking part in a relationship...

1. Unary Relationship: Only one participating entity For eg: employee Manages employee...
2. Binary Relationship: Only two participating entity For eg: Student takes course...
2. Ternary Relationship: Only three participating entity For eg: Employee works on a job and branch...

Relationship Constraints:

Mapping cardinality:

1. One-to-One: One entity in an entity set is related to only one entity from another entity set...
2. One-to-Many: One entity in an entity set is related to many entities in another entity set, but not many-to-one...
3. Many-to-One: Many entities in an entity set is related to only one entity in another entity set, but not One-to-Many...
4. Many-to-Many: Every entity in one entity set can be related to many entities in another entity set...

Participation Constraints: Also known as "Minimum Cardinality Constraint"...

1. Total Participation: Each entity in an entity set will have a relationship with an entity in another entity set...
2. Partial Participation: Each entity in an entity set need not have a relationship with an entity in another entity set...

For eg: Customer Borrows Loan

1. Here as Each loan is related to atleast one customer, its Total participation...
2. All Customers need not have a loan, therefore Partial participation...

Weak Entity have Total Participation Constraint...
Strong Entity may not always have Total Participation Constraint...

Type of Keys:

1. Super Key: Any no. of columns which can uniquely determine every row in the table.For eg: (EnrollmentNo,EnrollmentName)...
2. Candidate Key: Minimalistic version of super keys...
3. Primary Key: Any one key chosen from candidate keys and others are named as alternate keys...

Functional Dependency:

It is a relation in which a primary key tells what are the columns it can determine...
For eg: A -> B means A can determine every value in column B...

A is called determinant and B is called dependent...

Amstrong's Axioms:

1. Reflexive: If A and B are a set of attributes and A is a subset of B then A -> B will always hold true...
2. Augmentation: If A -> B, then AX -> BX...
3. Transitivity: If A -> B and B -> C, then A -> C always holds true...
4. Decomposition: If A -> BC, then A -> B and A -> C or vica-versa also holds true...
5. Composition: If A -> B and C -> D, then AC -> BD...

They are called sound because they only get us the correct FD's and complete because through correct FD's we
can get all correct FD's...

Why Normalization: It produces 3 anomalies(abnormalities):

1. Insertion: 
1.1. If you have merged two independent tables, then it will arise...
1.2. Because of that you will have to do more operations...

2. Deletion:
2.1. If you will delete one row from merged table, then you'll also be deleting important 
entry of other table as well...

3. Updation:
3.1. The updation which would have to be done at a single place might be required to be done 
at other places as well...

What we do in normalization:
We decompose a table into multiple tables until SRP(Single Responsibility Principle) is established...

Different Types of Normal Forms:

1. 1NF: Every attribute must have atomic values and not multivalued...
For eg: If name is an attribute then it shouldn't be the case that we have Madhav,Khushi in the
same row...

2. 2NF: 
2.1 Relation must be in 1 NF...
2.2 There should not be any partial dependency...

Partial Dependency: 
1. Every non-Prime attribute must be a dependent of prime attribute...
2. Any dependent should not be a dependent on part of a primary key...

3. 3NF:
3.1 Relation must be in 2 NF...
3.2 There should not be any transitivity dependency -> Non-Prime attribute should not be able to find 
Non-Prime attribute...

4. BCNF:
4.1 Relation must be in 3 NF...
3.2 For FD, A -> B, A must be a super key...

Transactions in DBMS:

Single task executed by DB...
Every transaction must follow ACID properties to mantain data integrity...

1. Atomicity ensures that either transaction is fully completed or not at all...
2. Consistency is clear by its name...
3. Isolation ensures concurrent data access for transactions(depends on isolation level)...

3.1. In Dirty Reads -> T1 sees uncommitted data of T2 and then T1 rolls back making data irrelevant...
3.2. In Non-repeatable reads -> For first time of query execution T1 sees some let's say value X of some column and another time 
some different value Y of same column...
3.3. In phantom Reads -> For first time of query execution T1 sees some let's say row X to have some values then another time some different 
row X to have some different values...

4. Durability: After successful transaction completion even if system fails changed data should persist in DB(Basically write should be performed
even after system failure)...

DB recovery mechanism component can have some of these algos for recovery:
1. Shadow copying based recovery: 
1.1 : Relies on old and new copy of the DB in RAM...
1.2 : Relies on a pointer to know the current copy of the DB...
1.3 : Transaction is considered complete only after new pointer is assigned...
1.4: Can't handle concurrent transactions...
1.5: Inefficient due to copying of the whole DB again and again...
1.6: We are using disk system for storing DB pointer which is atomic and durable... 

Transaction logs are written in a robust storage which ensures durability and atomicity...

2. Log Based Recovery:
2.1 : Logs are generated before every operation is performed...
2.2 : can use deferred DB modification using logs: First all logs are generated then acc. to the logs transaction is made...
2.3 : can use immediate DB modification using logs: First for a single step log is generated and then the step is made possible...

3. Checkpointing:
3.1 : Checkpoint represents last consistent state of the DB...
3.2 : Its stored in the form of transaction log or writing dirty pages info in DB...

Disadvantages: Performance overhead due to Disk I/O...

Indexing: Through indexing we are able to reduce Disk I/O which enhances the performance...
Index is a Data Structure which stores base pointer for all blocks with corresponding Primary Key on 
which when Binary Search is applied and then acc. to base pointer corresponding entry is find out...

Types of Indexing:
1. Primary Index: Index that is unique and sorted while clustering index is an index that is non-unique and sorted...
1.1. Sparse Index: For each and every block there is only one single entry in the index file -> used in primary indexing...
1.2. Dense Index: For each and every entry there is an entry in the index file -> used in clustering indexing...
1.3. If an attribute spans multiple blocks then we will have linked list of base pointers...

2. Secondary Index: Data file is unsorted and Dense Index happens...